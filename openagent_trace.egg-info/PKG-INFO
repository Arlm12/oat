Metadata-Version: 2.4
Name: openagent-trace
Version: 0.1.0
Summary: The Open Standard for AI Agent Observability
Author: OpenAgentTrace Contributors
License: MIT
Project-URL: Homepage, https://github.com/openagent-trace/openagent-trace
Project-URL: Documentation, https://docs.openagent-trace.dev
Project-URL: Repository, https://github.com/openagent-trace/openagent-trace
Project-URL: Issues, https://github.com/openagent-trace/openagent-trace/issues
Keywords: ai,agents,observability,tracing,llm,opentelemetry,monitoring
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: System :: Monitoring
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Provides-Extra: server
Requires-Dist: fastapi>=0.104.0; extra == "server"
Requires-Dist: uvicorn[standard]>=0.24.0; extra == "server"
Requires-Dist: pydantic>=2.5.0; extra == "server"
Requires-Dist: duckdb>=0.9.2; extra == "server"
Requires-Dist: python-multipart>=0.0.6; extra == "server"
Requires-Dist: websockets>=12.0; extra == "server"
Provides-Extra: openai
Requires-Dist: openai>=1.3.0; extra == "openai"
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.7.0; extra == "anthropic"
Provides-Extra: langchain
Requires-Dist: langchain>=0.1.0; extra == "langchain"
Provides-Extra: http
Requires-Dist: httpx>=0.25.0; extra == "http"
Provides-Extra: all
Requires-Dist: openagent-trace[anthropic,http,openai,server]; extra == "all"
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.7.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"

# ğŸ” OpenAgentTrace (OAT)

**The Open Standard for AI Agent Observability**

OpenAgentTrace is a vendor-neutral observability platform designed specifically for AI agents. Unlike traditional APM tools that focus on request-response patterns, OAT captures the **decision-making flow** of autonomous agentsâ€”every LLM call, tool execution, RAG retrieval, guardrail check, and agent handoff.

Think of it as **Grafana + Prometheus**, but built from the ground up for the AI agent era.

![Dashboard Preview](docs/dashboard.png)

## âœ¨ Key Features

- **ğŸ¯ Agent-Native Spans**: Semantic span types (`llm`, `tool`, `retrieval`, `guardrail`, `handoff`) that capture what agents actually do
- **ğŸ“Š DAG Visualization**: See your agent's reasoning flow as a directed graph, not just a timeline
- **ğŸ’° Cost Attribution**: Know exactly how much each component of your agent costs
- **âš¡ Zero-Config Instrumentation**: Auto-patches OpenAI, Anthropic, LangChain out of the box
- **ğŸ  Local-First**: Runs entirely on your machine with SQLite + DuckDB
- **ğŸ”Œ OpenTelemetry Compatible**: Fits into your existing observability stack

## ğŸš€ Quick Start

### 1. Install the SDK

```bash
pip install openagent-trace
# or
pip install -e .
```

### 2. Instrument Your Agent

```python
from oat import trace, span

@trace(span_type="llm")
async def call_llm(prompt: str):
    response = await openai.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response

@trace(span_type="tool")
def search_database(query: str):
    return db.search(query)

@trace(span_type="agent")
async def my_agent(user_input: str):
    # Your agent logic here
    plan = await call_llm(f"Plan for: {user_input}")
    results = search_database(plan)
    answer = await call_llm(f"Summarize: {results}")
    return answer
```

### 3. Start the Server & Dashboard

```bash
# Start the backend
cd server
uvicorn main:app --port 8787 --reload

# In another terminal, start the dashboard
cd ui
npm install
npm run dev
```

### 4. View Your Traces

Open [http://localhost:3000](http://localhost:3000) to see your agent's execution in real-time.

## ğŸ“ Project Structure

```
OpenAgentTrace/
â”œâ”€â”€ oat/                    # Python SDK
â”‚   â”œâ”€â”€ __init__.py         # Public API exports
â”‚   â”œâ”€â”€ tracer.py           # Core tracing logic, decorators
â”‚   â”œâ”€â”€ models.py           # Span/Trace data models
â”‚   â”œâ”€â”€ storage.py          # Local SQLite + blob storage
â”‚   â”œâ”€â”€ exporters.py        # HTTP, Console, File exporters
â”‚   â””â”€â”€ integrations/       # Auto-instrumentation
â”‚       â”œâ”€â”€ openai_integration.py
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ server/                 # FastAPI Backend
â”‚   â””â”€â”€ main.py             # API endpoints, DuckDB analytics
â”‚
â”œâ”€â”€ ui/                     # React Dashboard
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ pages/
â”‚       â”‚   â”œâ”€â”€ TracesPage.jsx      # Trace explorer
â”‚       â”‚   â”œâ”€â”€ TraceDetailPage.jsx # DAG + waterfall view
â”‚       â”‚   â””â”€â”€ AnalyticsPage.jsx   # Metrics & charts
â”‚       â””â”€â”€ components/
â”‚           â””â”€â”€ ...
â”‚
â””â”€â”€ examples/
    â””â”€â”€ demo_agent.py       # Example agent with all features
```

## ğŸ¨ SDK Usage

### Basic Tracing

```python
from oat import trace, span

# Decorator-based (recommended)
@trace(name="my_function", span_type="tool")
def my_function():
    return "result"

# Context manager (for more control)
with span("custom_operation", span_type="retrieval") as s:
    results = vector_db.search(query)
    s.set_output(results)
```

### Span Types

| Type | Description | Example |
|------|-------------|---------|
| `agent` | Root agent execution | `@trace(span_type="agent")` |
| `llm` | LLM inference call | GPT-4, Claude calls |
| `tool` | Tool/function execution | Web search, calculator |
| `retrieval` | RAG/vector search | Pinecone, Chroma queries |
| `guardrail` | Safety/validation check | Content moderation |
| `handoff` | Agent-to-agent transfer | Multi-agent systems |
| `embedding` | Embedding generation | text-embedding-3-small |
| `function` | Generic function (default) | Any traced function |

### Auto-Instrumentation

```python
from oat.integrations import patch_openai

# Call once at startup
patch_openai()

# All OpenAI calls are now automatically traced!
response = openai.chat.completions.create(...)
```

### LLM-Specific Tracing

```python
from oat import trace_llm

@trace_llm(model="gpt-4")
async def call_gpt(messages):
    return await openai.chat.completions.create(
        model="gpt-4",
        messages=messages
    )
# Automatically captures: tokens, cost, latency
```

### Manual Span Attributes

```python
from oat import span

with span("rag_search", span_type="retrieval") as s:
    s.retrieval_query = query
    s.retrieval_k = 5
    results = vectorstore.search(query, k=5)
    s.retrieval_scores = [r.score for r in results]
```

## ğŸ“ˆ Analytics API

The server exposes analytics endpoints powered by DuckDB:

```bash
# Overview metrics
GET /analytics/overview?hours=24

# Latency percentiles by span type
GET /analytics/latency?hours=24

# Response:
{
  "overview": {
    "trace_count": 150,
    "span_count": 1234,
    "error_rate": 2.5,
    "p95_duration_ms": 1250.5,
    "total_tokens": 50000,
    "total_cost": 1.2345
  },
  "by_type": [...],
  "by_model": [...],
  "timeseries": [...]
}
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# SDK Configuration
OAT_SERVICE_NAME=my-agent
OAT_EXPORT_URL=http://localhost:8787
OAT_DATA_DIR=.oat

# Server Configuration
OAT_SQLITE_PATH=.oat/traces.db
OAT_DUCKDB_PATH=.oat/analytics.duckdb
```

### Programmatic Configuration

```python
from oat import get_tracer

tracer = get_tracer(
    service_name="my-agent",
    data_dir=".oat",
    export_url="http://localhost:8787",
    auto_flush=True,
    flush_interval=1.0
)
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Clone the repo
git clone https://github.com/your-org/OpenAgentTrace.git
cd OpenAgentTrace

# Install in development mode
pip install -e ".[dev]"

# Run tests
pytest

# Start development servers
cd server && uvicorn main:app --reload --port 8787 &
cd ui && npm run dev
```

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

Built with inspiration from:
- [OpenTelemetry](https://opentelemetry.io/) - The observability standard
- [LangSmith](https://smith.langchain.com/) - LangChain's tracing solution  
- [Arize Phoenix](https://phoenix.arize.com/) - AI observability platform

---

**Made with â¤ï¸ by the OpenAgentTrace community**
